# TODO List
* You should do split before any tokenization stuff, remove ability to have one single full data file
* Explain pipeline in readme: tokenizer is trained, all the data is tokenized before trraining, and padding is added dynamically during training.
* Lazy load data in dataset class
* Add test step function to model
* Add better inference code, better default metrics in inference script
* Add better logging. Tensorboard?
* Generate requirements txt
* Cleanup YAML
* cleanup/organize code
* Test loading/training pretrained model
* Test